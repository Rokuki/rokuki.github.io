<!DOCTYPE html>

<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=0">

  <meta name="description" content="Data StorageIf you have an SSD, then you can store your data on the SSD instead of HDD. Loading from .npy files can also be faster than from .png beca">


	<meta name="keywords" content="deep learning, PyTorch, Python, Computer vision">

<link rel="alternate" href="/atom.xml" title="BouSeng" type="application/atom+xml">
<meta name="theme-color" content="#a1d0f6">
<title>Train faster(PyTorch) - BouSeng</title>
<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
<link rel="shortcut icon" href="/favicon.png">
<link rel="stylesheet" href="/css/style.css">
<nav class="main-nav">
	
	    <a href="/">← 主页</a>
	
	
	    <a href="/about/">About</a>
	
	    <a href="/archives/">Archives</a>
	
	<a class="cta" href="/atom.xml" data-no-instant>订阅</a>
</nav>

<section id="wrapper">
    <article class="post">
    <header>
        
            <h1>Train faster(PyTorch)</h1>
        
        <h2 class="headline">Oct 16 2019
        
        </h2>
    </header>
</article>
<section id="post-body"><h2 id="Data-Storage"><a href="#Data-Storage" class="headerlink" title="Data Storage"></a>Data Storage</h2><p>If you have an SSD, then you can store your data on the SSD instead of HDD. Loading from <code>.npy</code> files can also be faster than from <code>.png</code> because it avoids CPU conversion. You can preprocess the dataset into some <code>.npy</code> matrix before training.</p>
<h2 id="Code-writing-style"><a href="#Code-writing-style" class="headerlink" title="Code writing style"></a>Code writing style</h2><p>Use</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x)</span>:</span></span><br><span class="line">  x = conv1(x)</span><br><span class="line">  x = conv2(x)</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>instead of</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x)</span>:</span></span><br><span class="line">  x1 = conv1(x)</span><br><span class="line">  x2 = conv2(x1)</span><br><span class="line">  ... <span class="keyword">return</span> x34</span><br></pre></td></tr></table></figure>

<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>Using <code>nn.ReLU(inplace=True)</code> might reduce memory usage, but it might also introduce an extra step of gradient calculation.</p>
<h2 id="Half-Precision-Training"><a href="#Half-Precision-Training" class="headerlink" title="Half Precision Training"></a>Half Precision Training</h2><p>Use <code>APEX</code> for FP16 training: <a href="https://github.com/NVIDIA/apex" target="_blank" rel="noopener">https://github.com/NVIDIA/apex</a><br>APEX also supports multi-GPU parallel.</p>
<h2 id="prefetch-generator"><a href="#prefetch-generator" class="headerlink" title="prefetch_generator"></a>prefetch_generator</h2><p>install</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install prefetch_generator</span><br></pre></td></tr></table></figure>

<p>Usage</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> prefetch_generator <span class="keyword">import</span> BackgroundGenerator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataLoaderX</span><span class="params">(DataLoader)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> BackgroundGenerator(super().__iter__())</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">train_loader = DataLoaderX(dataset=train_dataset,</span><br><span class="line">                                           batch_size=batch_size,</span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="「幽灵」批归一化"><a href="#「幽灵」批归一化" class="headerlink" title="「幽灵」批归一化"></a>「幽灵」批归一化</h2><p>批归一化（BN）似乎在批量大小为 32 的时候效果最好，而研究者的批大小为 512。此外，如果不想严重影响训练效果，那么他们就不能降低批量大小。因此，研究者将批归一化独立地应用到各批量数据的子集中。这一技术，就被称之为「幽灵」批归一化，它通常用于分布式训练中，但如果单节点运算的批量数据太大，那么也能用这样的技术。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/97190313" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/97190313</a></p>
</section>
    
        
        <h2 class="footline">
            <a href="/tags/PyTorch/#PyTorch">PyTorch</a>, <a href="/tags/tricks/#tricks">tricks</a>
        </h2>
    

    <footer id="post-meta" class="clearfix">
        <a href="/about/">
        <img class="avatar" src="/images/avatar.png">
        <div>
            <span class="dark">BouSeng</span>
            <span>What I cannot create, I do not understand</span>
        </div>
        </a>
        <section id="sharing">
            <a title="Share to Twitter" class="twitter" href="https://twitter.com/intent/tweet?text=bouseng.com/2019/10/16/Train-faster-PyTorch/ - Train faster(PyTorch) @"><span class="icon-twitter">tweet</span></a>
            <a title="Share to Facebook" class="facebook" href="#" onclick="
                window.open(
                  'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
                  'facebook-share-dialog',
                  'width=626,height=436');
                return false;"><span class="icon-facebook-sign">Share</span>
            </a>
        </section>
    </footer>


  <section id="comment">
    <button class="btn" id="loadcmts" onclick="cmts.load();">加载评论</button>
    <div id="gitment"></div>
    <script src='/js/gitment.browser.js'></script>
    <link rel="stylesheet" href=''>
    <script>
      var cmts={
        load:function cmts(){
          var gitment = new Gitment({
          
            id: "Train faster(PyTorch)",
          
            owner: "",
            repo: "",
            oauth: {
              client_id: "",
              client_secret: "",
            },
          })
          gitment.render('gitment');
          var loadcmt = document.getElementById("loadcmts");
          var imyourfather = loadcmt.parentNode;
          imyourfather.removeChild(loadcmts)
        }
      }
    </script>
  </section>


	<footer id="footer">
	<div id="social">
		<p class="small">©  BouSeng Chan| Powered by Hexo & 
			<a href="https://github.com/F0r3at/Lights"> Lights</a>
		</p>
	</div>
</footer>

</section>

	<script src="//cdnjs.loli.net/ajax/libs/instantclick/3.0.1/instantclick.min.js" data-no-instant></script>
	<script data-no-instant>
		
		InstantClick.init('mousedown');
	</script>



