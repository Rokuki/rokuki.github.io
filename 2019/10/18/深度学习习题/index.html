<!DOCTYPE html>

<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=0">

  <meta name="description" content="1.计算下面函数的一阶导数和二阶导数： 2.计算下面两个向量的内积： 3.计算下面向量的1范数和2范数： 4.计算下面两个矩阵的乘积： 5.计算下面多元函数的偏导数： 6.计算下面多元函数的梯度： 7.计算下面多元函数的雅克比矩阵： 8.计算下面多元函数的Hessian矩阵： 9.计算下面函数的所有">


	<meta name="keywords" content="deep learning, PyTorch, Python, Computer vision">

<link rel="alternate" href="/atom.xml" title="BouSeng" type="application/atom+xml">
<meta name="theme-color" content="#a1d0f6">
<title>深度学习习题 - BouSeng</title>
<!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
<link rel="shortcut icon" href="/favicon.png">
<link rel="stylesheet" href="/css/style.css">
<nav class="main-nav">
	
	    <a href="/">← 主页</a>
	
	
	    <a href="/about/">About</a>
	
	    <a href="/archives/">Archives</a>
	
	<a class="cta" href="/atom.xml" data-no-instant>订阅</a>
</nav>

<section id="wrapper">
    <article class="post">
    <header>
        
            <h1>深度学习习题</h1>
        
        <h2 class="headline">Oct 18 2019
        
        </h2>
    </header>
</article>
<section id="post-body"><div class="RichText ztext Post-RichText">
1.计算下面函数的一阶导数和二阶导数：<p></p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=f%28x%29%3Dx+%5Cln+x-%5Cfrac%7B1%2B%5Cexp+%282+x%29%7D%7B1-%5Cexp+%282+x%29%7D" alt="[公式]" eeimg="1" data-formula="f(x)=x \ln x-\frac{1+\exp (2 x)}{1-\exp (2 x)}"> </p><p class="ztext-empty-paragraph"><br></p><p>2.计算下面两个向量的内积：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmathbf%7Bx%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Blll%7D%7B1%7D+%26+%7B2%7D+%26+%7B3%7D%5Cend%7Barray%7D%5Cright%5D%7D+%5C%5C+%7B%5Cmathbf%7By%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Blll%7D%7B-1%7D+%26+%7B5%7D+%26+%7B10%7D%5Cend%7Barray%7D%5Cright%5D%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\mathbf{x}=\left[\begin{array}{lll}{1} &amp; {2} &amp; {3}\end{array}\right]} \\ {\mathbf{y}=\left[\begin{array}{lll}{-1} &amp; {5} &amp; {10}\end{array}\right]}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p>3.计算下面向量的1范数和2范数：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Blll%7D%7B1%7D+%26+%7B-2%7D+%26+%7B3%7D%5Cend%7Barray%7D%5Cright%5D" alt="[公式]" eeimg="1" data-formula="\mathbf{x}=\left[\begin{array}{lll}{1} &amp; {-2} &amp; {3}\end{array}\right]"> </p><p class="ztext-empty-paragraph"><br></p><p>4.计算下面两个矩阵的乘积：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmathbf%7BA%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Blll%7D%7B1%7D+%26+%7B2%7D+%26+%7B1%7D+%5C%5C+%7B0%7D+%26+%7B1%7D+%26+%7B1%7D%5Cend%7Barray%7D%5Cright%5D%7D+%5C%5C+%7BB%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Bllll%7D%7B1%7D+%26+%7B2%7D+%26+%7B1%7D+%26+%7B1%7D+%5C%5C+%7B5%7D+%26+%7B4%7D+%26+%7B0%7D+%26+%7B1%7D+%5C%5C+%7B7%7D+%26+%7B6%7D+%26+%7B1%7D+%26+%7B0%7D%5Cend%7Barray%7D%5Cright%5D%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\mathbf{A}=\left[\begin{array}{lll}{1} &amp; {2} &amp; {1} \\ {0} &amp; {1} &amp; {1}\end{array}\right]} \\ {B=\left[\begin{array}{llll}{1} &amp; {2} &amp; {1} &amp; {1} \\ {5} &amp; {4} &amp; {0} &amp; {1} \\ {7} &amp; {6} &amp; {1} &amp; {0}\end{array}\right]}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p>5.计算下面多元函数的偏导数：</p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=f%5Cleft%28x_%7B1%7D%2C+x_%7B2%7D%2C+x_%7B3%7D%5Cright%29%3D%5Cln+%5Cleft%281%2B%5Cexp+%5Cleft%28-2+x_%7B1%7D%2B3+x_%7B2%7D-4+x_%7B3%7D%5Cright%29%5Cright%29" alt="[公式]" eeimg="1" data-formula="f\left(x_{1}, x_{2}, x_{3}\right)=\ln \left(1+\exp \left(-2 x_{1}+3 x_{2}-4 x_{3}\right)\right)"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>6.计算下面多元函数的梯度：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=f%5Cleft%28x_%7B1%7D%2C+x_%7B2%7D%2C+x_%7B3%7D%5Cright%29%3D%5Cln+%5Cleft%281%2B%5Cexp+%5Cleft%28-2+x_%7B1%7D%5E%7B2%7D%2B3+x_%7B2%7D%5E%7B3%7D-4+x_%7B3%7D%5Cright%29%5Cright%29" alt="[公式]" eeimg="1" data-formula="f\left(x_{1}, x_{2}, x_{3}\right)=\ln \left(1+\exp \left(-2 x_{1}^{2}+3 x_{2}^{3}-4 x_{3}\right)\right)"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>7.计算下面多元函数的雅克比矩阵：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=f%5Cleft%28x_%7B1%7D%2C+x_%7B2%7D%2C+x_%7B3%7D%5Cright%29%3Dx_%7B1%7D%5E%7B2%7D-%5Cln+x_%7B2%7D%2B%5Cexp+%5Cleft%28x_%7B1%7D+x_%7B3%7D%5Cright%29" alt="[公式]" eeimg="1" data-formula="f\left(x_{1}, x_{2}, x_{3}\right)=x_{1}^{2}-\ln x_{2}+\exp \left(x_{1} x_{3}\right)"> </p><p class="ztext-empty-paragraph"><br></p><p>8.计算下面多元函数的Hessian矩阵：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=f%5Cleft%28x_%7B1%7D%2C+x_%7B2%7D%2C+x_%7B3%7D%5Cright%29%3Dx_%7B1%7D%5E%7B2%7D-%5Cln+x_%7B2%7D%2B%5Cexp+%5Cleft%28x_%7B1%7D+x_%7B3%7D%5Cright%29" alt="[公式]" eeimg="1" data-formula="f\left(x_{1}, x_{2}, x_{3}\right)=x_{1}^{2}-\ln x_{2}+\exp \left(x_{1} x_{3}\right)"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>9.计算下面函数的所有极值点，并指明是极大值还是极小值：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=f%28x%29%3Dx%5E%7B3%7D%2B2+x%5E%7B2%7D-5+x%2B10" alt="[公式]" eeimg="1" data-formula="f(x)=x^{3}+2 x^{2}-5 x+10"> </p><p class="ztext-empty-paragraph"><br></p><p>10.推导多元函数梯度下降法的迭代公式。</p><p class="ztext-empty-paragraph"><br></p><p>11.梯度下降法为什么要在迭代公式中使用步长系数？</p><p class="ztext-empty-paragraph"><br></p><p>12.梯度下降法如何判断是否收敛？</p><p class="ztext-empty-paragraph"><br></p><p>13.推导多元函数牛顿法的迭代公式。</p><p class="ztext-empty-paragraph"><br></p><p>14.如果步长系数充分小，牛顿法在每次迭代时能保证函数值下降吗？</p><p class="ztext-empty-paragraph"><br></p><p>15.梯度下降法和牛顿法能保证找到函数的极小值点吗，为什么？</p><p class="ztext-empty-paragraph"><br></p><p>16.解释一元函数极值判别法则。</p><p class="ztext-empty-paragraph"><br></p><p>17.解释多元函数极值判别法则。</p><p class="ztext-empty-paragraph"><br></p><p>18.什么是鞍点？</p><p class="ztext-empty-paragraph"><br></p><p>19.解释什么是局部极小值，什么是全局极小值。</p><p class="ztext-empty-paragraph"><br></p><p>20.用拉格朗日乘数法求解如下极值问题</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmin+f%5Cleft%28x_%7B1%7D%2C+x_%7B2%7D%5Cright%29%3Dx_%7B1%7D%5E%7B2%7D-4+x_%7B1%7D+x_%7B2%7D%2Bx_%7B2%7D%5E%7B2%7D%7D+%5C%5C+%7Bx_%7B1%7D%2Bx_%7B2%7D%2Bx_%7B3%7D%3D1%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\min f\left(x_{1}, x_{2}\right)=x_{1}^{2}-4 x_{1} x_{2}+x_{2}^{2}} \\ {x_{1}+x_{2}+x_{3}=1}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p>21.什么是凸集？</p><p class="ztext-empty-paragraph"><br></p><p>22.什么是凸函数，如何判断一个一元函数是不是凸函数，如何判断一个多元函数是不是凸函数？</p><p class="ztext-empty-paragraph"><br></p><p>22.什么是凸优化？</p><p class="ztext-empty-paragraph"><br></p><p>23.证明凸优化问题的局部最优解一定是全局最优解。</p><p class="ztext-empty-paragraph"><br></p><p>24.对于如下最优化问题：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmin+f%5Cleft%28x_%7B1%7D%2C+x_%7B2%7D%5Cright%29%3Dx_%7B1%7D%5E%7B2%7D-4+x_%7B1%7D+x_%7B2%7D%2Bx_%7B2%7D%5E%7B2%7D%7D+%5C%5C+%7Bx_%7B1%7D%2Bx_%7B2%7D%2Bx_%7B3%7D%3D1%7D+%5C%5C+%7Bx_%7B1%7D-x_%7B2%7D%2Bx_%7B3%7D%3E0%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\min f\left(x_{1}, x_{2}\right)=x_{1}^{2}-4 x_{1} x_{2}+x_{2}^{2}} \\ {x_{1}+x_{2}+x_{3}=1} \\ {x_{1}-x_{2}+x_{3}>0}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p>构造广义拉格朗日乘子函数，将该问题转化为对偶问题。</p><p class="ztext-empty-paragraph"><br></p><p>25.一维正态分布的概率密度函数为</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B%5Csqrt%7B2+%5Cpi%7D+%5Csigma%7D+%5Cexp+%5Cleft%28-%5Cfrac%7B%28x-%5Cmu%29%5E%7B2%7D%7D%7B2+%5Csigma%5E%7B2%7D%7D%5Cright%29" alt="[公式]" eeimg="1" data-formula="\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)"> </p><p class="ztext-empty-paragraph"><br></p><p>给定一组样本 <img src="https://www.zhihu.com/equation?tex=x_%7B1%7D%2C...x_%7Bl%7D" alt="[公式]" eeimg="1" data-formula="x_{1},...x_{l}"> 。用最大似然估计求解正态分布的均值和方差。</p><p class="ztext-empty-paragraph"><br></p><p>26.如何判断一个矩阵是否为正定矩阵？</p><p class="ztext-empty-paragraph"><br></p><p>27.解释最速下降法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>28.解释坐标下降法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>29.一维正态分布的概率密度函数为</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B%5Csqrt%7B2+%5Cpi%7D+%5Csigma%7D+%5Cexp+%5Cleft%28-%5Cfrac%7B%28x-%5Cmu%29%5E%7B2%7D%7D%7B2+%5Csigma%5E%7B2%7D%7D%5Cright%29" alt="[公式]" eeimg="1" data-formula="\frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)"> </p><p class="ztext-empty-paragraph"><br></p><p>按照定义计算其数学期望与方差。</p><p class="ztext-empty-paragraph"><br></p><p>30.两个离散型概率分布的KL散度定义为：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=D_%7B%5Cmathrm%7BKL%7D%7D%28P+%5C%7C+Q%29%3D%5Csum_%7Bx%7D+P%28x%29+%5Cln+%5Cfrac%7BP%28x%29%7D%7BQ%28x%29%7D" alt="[公式]" eeimg="1" data-formula="D_{\mathrm{KL}}(P \| Q)=\sum_{x} P(x) \ln \frac{P(x)}{Q(x)}"> </p><p class="ztext-empty-paragraph"><br></p><p>利用下面的不等式，当x&gt;0时：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cln+x+%5Cleq+x-1" alt="[公式]" eeimg="1" data-formula="\ln x \leq x-1"> </p><p class="ztext-empty-paragraph"><br></p><p>证明KL散度非负，即</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=D_%7B%5Cmathrm%7BKL%7D%7D%28p+%5C%7C+q%29+%5Cgeq+0" alt="[公式]" eeimg="1" data-formula="D_{\mathrm{KL}}(p \| q) \geq 0"> </p><p class="ztext-empty-paragraph"><br></p><p>31.对于离散型概率分布，证明当其为均匀分布时熵有最大值。</p><p class="ztext-empty-paragraph"><br></p><p>32.对于连续型概率分布，已知其数学期望为 ，方差为 。用变分法证明当此分布为正态分布时熵有最大值。</p><p class="ztext-empty-paragraph"><br></p><p>33.对于两个离散型概率分布，证明当二者相等时交叉熵有极小值。</p><p class="ztext-empty-paragraph"><br></p><p>34.为什么在实际的机器学习应用中经常假设样本数据服从正态分布？</p><p class="ztext-empty-paragraph"><br></p><p>35.什么是随机事件独立，什么是随机向量独立？</p><p class="ztext-empty-paragraph"><br></p><p>36.什么是弱对偶？什么是强对偶？</p><p class="ztext-empty-paragraph"><br></p><p>37.证明弱对偶定理。</p><p class="ztext-empty-paragraph"><br></p><p>38.简述Slater条件。</p><p class="ztext-empty-paragraph"><br></p><p>39.简述KKT条件。</p><p class="ztext-empty-paragraph"><br></p><p>40.解释蒙特卡洛算法的原理。为什么蒙特卡洛算法能够收敛？</p><p class="ztext-empty-paragraph"><br></p><p>41.解释熵概念。</p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>1.名词解释：有监督学习，无监督学习，半监督学习。</p><p class="ztext-empty-paragraph"><br></p><p>2.列举常见的有监督学习算法。</p><p class="ztext-empty-paragraph"><br></p><p>3.列举常见的无监督学习算法。</p><p class="ztext-empty-paragraph"><br></p><p>4.简述强化学习的原理。</p><p class="ztext-empty-paragraph"><br></p><p>5.什么是生成模型？什么是判别模型？</p><p class="ztext-empty-paragraph"><br></p><p>6.概率模型一定是生成模型吗？</p><p class="ztext-empty-paragraph"><br></p><p>7.不定项选择。下面那些算法是生成模型？___________哪些算法是判别模型？__________</p><p>A.决策树    B.贝叶斯分类器    C.全连接神经网络  D.支持向量机   E. logistic回归</p><p>F. AdaBoost算法   G.隐马尔可夫模型    H.条件随机场    I.受限玻尔兹曼机</p><p class="ztext-empty-paragraph"><br></p><p>8.如何判断是否发生过拟合？</p><p class="ztext-empty-paragraph"><br></p><p>9.发生过拟合的原因有哪些，应该怎么解决？</p><p class="ztext-empty-paragraph"><br></p><p>10.列举常见的正则化方法。</p><p class="ztext-empty-paragraph"><br></p><p>11.解释ROC曲线的原理。</p><p class="ztext-empty-paragraph"><br></p><p>12.解释精度，召回率，F1值的定义。</p><p class="ztext-empty-paragraph"><br></p><p>13.解释交叉验证的原理。</p><p class="ztext-empty-paragraph"><br></p><p>14.什么是过拟合，什么是欠拟合？</p><p class="ztext-empty-paragraph"><br></p><p>15.什么是没有免费午餐定理？</p><p class="ztext-empty-paragraph"><br></p><p>16.简述奥卡姆剃刀原理。</p><p class="ztext-empty-paragraph"><br></p><p>17.推导偏差-方差分解公式。</p><p class="ztext-empty-paragraph"><br></p><p>18.证明如果采用均方误差函数，线性回归的优化问题是凸优化问题。</p><p class="ztext-empty-paragraph"><br></p><p>19.推导线性回归的梯度下降迭代公式。</p><p class="ztext-empty-paragraph"><br></p><p>20.解释混淆矩阵的概念。</p><p class="ztext-empty-paragraph"><br></p><p>21.解释岭回归的原理。</p><p class="ztext-empty-paragraph"><br></p><p>22.解释LASSO回归的原理。</p><p class="ztext-empty-paragraph"><br></p><h2><b>第4章 贝叶斯分类器</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.什么是先验概率，什么是后验概率？</p><p class="ztext-empty-paragraph"><br></p><p>2.推导朴素贝叶斯分类器的预测函数。</p><p class="ztext-empty-paragraph"><br></p><p>3.什么是拉普拉斯光滑？</p><p class="ztext-empty-paragraph"><br></p><p>4.推导正态贝叶斯分类器的预测函数。</p><p class="ztext-empty-paragraph"><br></p><p>5.贝叶斯分类器是生成模型还是判别模型？</p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><h2><b>第5章 决策树</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.什么是预剪枝，什么是后剪枝？</p><p class="ztext-empty-paragraph"><br></p><p>2.什么是属性缺失问题？</p><p class="ztext-empty-paragraph"><br></p><p>3.对于属性缺失问题，在训练时如何生成替代分裂规则？</p><p class="ztext-empty-paragraph"><br></p><p>4.列举分类问题的分裂评价指标。</p><p class="ztext-empty-paragraph"><br></p><p>5.证明当各个类出现的概率相等时，Gini不纯度有极大值；当样本全部属于某一类时，Gini不纯度有极小值。</p><p class="ztext-empty-paragraph"><br></p><p>6.ID3用什么指标作为分裂的评价指标？</p><p class="ztext-empty-paragraph"><br></p><p>7.C4.5用什么指标作为分裂的评价指标？</p><p class="ztext-empty-paragraph"><br></p><p>8.解释决策树训练时寻找最佳分裂的原理。</p><p class="ztext-empty-paragraph"><br></p><p>9.对于分类问题，叶子节点的值如何设定？对于回归问题，决策树叶子节点的值如何设定？</p><p class="ztext-empty-paragraph"><br></p><p>10.决策树如何计算特征的重要性？</p><p class="ztext-empty-paragraph"><br></p><p>11.CART对分类问题和回归问题分别使用什么作为分裂评价指标？</p><p class="ztext-empty-paragraph"><br></p><h2><b>第6章 k近邻算法与距离度量学习</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.简述k近邻算法的预测算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>2.简述k的取值对k近邻算法的影响。</p><p class="ztext-empty-paragraph"><br></p><p>3.距离函数需要满足哪些数学条件？</p><p class="ztext-empty-paragraph"><br></p><p>4.列举常见的距离函数。</p><p class="ztext-empty-paragraph"><br></p><p>5.解释距离度量学习的原理。</p><p class="ztext-empty-paragraph"><br></p><p>6.解释LMNN算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>7.解释ITML算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>8.解释NCA算法的原理。</p><p class="ztext-empty-paragraph"><br></p><h2><b>第7章 数据降维</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.使用数据降维算法的目的是什么？</p><p class="ztext-empty-paragraph"><br></p><p>2.列举常见的数据降维算法。</p><p class="ztext-empty-paragraph"><br></p><p>3.常见的降维算法中，哪些是监督降维，哪些是无监督降维？</p><p class="ztext-empty-paragraph"><br></p><p>4.什么是流形？</p><p class="ztext-empty-paragraph"><br></p><p>5.根据最小化重构误差准则推导PCA投影矩阵的计算公式。</p><p class="ztext-empty-paragraph"><br></p><p>6.解释PCA降维算法的流程。</p><p class="ztext-empty-paragraph"><br></p><p>7.解释PCA重构算法的流程。</p><p class="ztext-empty-paragraph"><br></p><p>8.解释LLE的原理。</p><p class="ztext-empty-paragraph"><br></p><p>9.名词解释：图的拉普拉斯矩阵。</p><p class="ztext-empty-paragraph"><br></p><p>10.解释t-SNE的原理。</p><p class="ztext-empty-paragraph"><br></p><p>11.解释KPCA的原理。</p><p class="ztext-empty-paragraph"><br></p><p>12.证明图的拉普拉斯矩阵半正定。</p><p class="ztext-empty-paragraph"><br></p><p>13.解释拉普拉斯特征映射的原理。</p><p class="ztext-empty-paragraph"><br></p><p>14.解释等距映射的与原理。</p><p class="ztext-empty-paragraph"><br></p><p>15.PCA是有监督学习还是无监督学习？</p><p class="ztext-empty-paragraph"><br></p><h2><b>第8章 线性判别分析</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.解释LDA的原理。</p><p class="ztext-empty-paragraph"><br></p><p>2.推导多类和高维时LDA的投影矩阵计算公式。</p><p class="ztext-empty-paragraph"><br></p><p>3.解释LDA降维算法的流程。</p><p class="ztext-empty-paragraph"><br></p><p>4.解释LDA重构算法的流程。</p><p class="ztext-empty-paragraph"><br></p><p>5.LDA是有监督学习还是无监督学习？</p><p class="ztext-empty-paragraph"><br></p><h2><b>第9章 人工神经网络</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.神经网络为什么需要激活函数？</p><p class="ztext-empty-paragraph"><br></p><p>2.推导sigmoid函数的导数计算公式。</p><p class="ztext-empty-paragraph"><br></p><p>3.激活函数需要满足什么数学条件？</p><p class="ztext-empty-paragraph"><br></p><p>4.为什么激活函数只要求几乎处处可导而不需要在所有点处可导？</p><p class="ztext-empty-paragraph"><br></p><p>5.什么是梯度消失问题，为什么会出现梯度消失问题？</p><p class="ztext-empty-paragraph"><br></p><p>6.如果特征向量中有类别型特征，使用神经网络时应该如何处理？</p><p class="ztext-empty-paragraph"><br></p><p>7.对于多分类问题，神经网络的输出值应该如何设计？</p><p class="ztext-empty-paragraph"><br></p><p>8.神经网络参数的初始值如何设定？</p><p class="ztext-empty-paragraph"><br></p><p>9.如果采用欧氏距离损失函数，推导输出层的梯度值。推导隐含层参数梯度的计算公式。</p><p class="ztext-empty-paragraph"><br></p><p>10.如果采用softmax+交叉熵的方案，推导损失函数对softmax输入变量的梯度值。</p><p class="ztext-empty-paragraph"><br></p><p>11.解释动量项的原理。</p><p class="ztext-empty-paragraph"><br></p><p>12.列举神经网络的正则化技术。</p><p class="ztext-empty-paragraph"><br></p><p>13.推导ReLU函数导数计算公式。</p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><h2><b>第10章 支持向量机</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.推导线性可分时SVM的原问题：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmin+%5Cfrac%7B1%7D%7B2%7D+%5Cmathrm%7Bw%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathrm%7Bw%7D%7D+%5C%5C+%7By_%7Bi%7D%5Cleft%28%5Cmathrm%7Bw%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathrm%7Bx%7D_%7Bi%7D%2Bb%5Cright%29+%5Cgeq+1%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\min \frac{1}{2} \mathrm{w}^{\mathrm{T}} \mathrm{w}} \\ {y_{i}\left(\mathrm{w}^{\mathrm{T}} \mathrm{x}_{i}+b\right) \geq 1}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p>2.证明线性可分时SVM的原问题是凸优化问题且Slater条件成立：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmin+%5Cfrac%7B1%7D%7B2%7D+%5Cmathrm%7Bw%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathrm%7Bw%7D%7D+%5C%5C+%7By_%7Bi%7D%5Cleft%28%5Cmathrm%7Bw%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathrm%7Bx%7D_%7Bi%7D%2Bb%5Cright%29+%5Cgeq+1%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\min \frac{1}{2} \mathrm{w}^{\mathrm{T}} \mathrm{w}} \\ {y_{i}\left(\mathrm{w}^{\mathrm{T}} \mathrm{x}_{i}+b\right) \geq 1}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>3.推导线性可分时SVM的对偶问题：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmin+_%7B%5Calpha%7D+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Csum_%7Bj%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bi%7D+%5Calpha_%7Bj%7D+y_%7Bi%7D+y_%7Bj%7D+%5Cmathbf%7Bx%7D_%7Bi%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathbf%7Bx%7D_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bi%7D%7D+%5C%5C+%7B%5Calpha_%7Bi%7D+%5Cgeq+0%2C+i%3D1%2C+%5Cldots%2C+l%7D+%5C%5C+%7B%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bi%7D+y_%7Bi%7D%3D0%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\min _{\alpha} \frac{1}{2} \sum_{i=1}^{l} \sum_{j=1}^{l} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathbf{x}_{i}^{\mathrm{T}} \mathbf{x}_{j}-\sum_{i=1}^{l} \alpha_{i}} \\ {\alpha_{i} \geq 0, i=1, \ldots, l} \\ {\sum_{i=1}^{l} \alpha_{i} y_{i}=0}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>4.证明加入松弛变量和惩罚因子之后，SVM的原问题是凸优化问题且Slater条件成立：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmin+%5Cfrac%7B1%7D%7B2%7D+%5Cmathbf%7Bw%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathbf%7Bw%7D%2BC+%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Cxi_%7Bi%7D%7D+%5C%5C+%7By_%7Bi%7D%5Cleft%28%5Cmathbf%7Bw%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathbf%7Bx%7D_%7Bi%7D%2Bb%5Cright%29+%5Cgeq+1-%5Cxi_%7Bi%7D%7D+%5C%5C+%7B%5Cxi_%7Bi%7D+%5Cgeq+0%2C+i%3D1%2C+%5Cldots%2C+l%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\min \frac{1}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}+C \sum_{i=1}^{l} \xi_{i}} \\ {y_{i}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{i}+b\right) \geq 1-\xi_{i}} \\ {\xi_{i} \geq 0, i=1, \ldots, l}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>5.推导线性不可分时SVM的对偶问题：</p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmin+_%7B%5Calpha%7D+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Csum_%7Bj%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bi%7D+%5Calpha_%7Bj%7D+y_%7Bi%7D+y_%7Bj%7D+%5Cmathrm%7Bx%7D_%7Bi%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathrm%7Bx%7D_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bi%7D%7D+%5C%5C+%7B0+%5Cleq+%5Calpha_%7Bi%7D+%5Cleq+C%7D+%5C%5C+%7B%5Csum_%7Bj%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bj%7D+y_%7Bj%7D%3D0%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\min _{\alpha} \frac{1}{2} \sum_{i=1}^{l} \sum_{j=1}^{l} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathrm{x}_{i}^{\mathrm{T}} \mathrm{x}_{j}-\sum_{i=1}^{l} \alpha_{i}} \\ {0 \leq \alpha_{i} \leq C} \\ {\sum_{j=1}^{l} \alpha_{j} y_{j}=0}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p>6.证明线性不可分时SVM的对偶问题是凸优化问题：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmin+_%7B%5Calpha%7D+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Csum_%7Bj%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bi%7D+%5Calpha_%7Bj%7D+y_%7Bi%7D+y_%7Bj%7D+%5Cmathbf%7Bx%7D_%7Bi%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathbf%7Bx%7D_%7Bj%7D-%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bi%7D%7D+%5C%5C+%7B0+%5Cleq+%5Calpha_%7Bi%7D+%5Cleq+C%7D+%5C%5C+%7B%5Csum_%7Bj%3D1%7D%5E%7Bl%7D+%5Calpha_%7Bj%7D+y_%7Bj%7D%3D0%7D%5Cend%7Barray%7D" alt="[公式]" eeimg="1" data-formula="\begin{array}{l}{\min _{\alpha} \frac{1}{2} \sum_{i=1}^{l} \sum_{j=1}^{l} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathbf{x}_{i}^{\mathrm{T}} \mathbf{x}_{j}-\sum_{i=1}^{l} \alpha_{i}} \\ {0 \leq \alpha_{i} \leq C} \\ {\sum_{j=1}^{l} \alpha_{j} y_{j}=0}\end{array}"> </p><p class="ztext-empty-paragraph"><br></p><p>7.用KKT条件证明SVM所有样本满足如下条件：</p><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic3.zhimg.com/v2-5efcb37cba3962cef377fabce53b996e_b.jpg" data-caption data-size="normal" data-rawwidth="423" data-rawheight="199" class="origin_image zh-lightbox-thumb" width="423" data-original="https://pic3.zhimg.com/v2-5efcb37cba3962cef377fabce53b996e_r.jpg"></noscript><img src="https://pic3.zhimg.com/80/v2-5efcb37cba3962cef377fabce53b996e_hd.jpg" data-caption data-size="normal" data-rawwidth="423" data-rawheight="199" class="origin_image zh-lightbox-thumb lazy" width="423" data-original="https://pic3.zhimg.com/v2-5efcb37cba3962cef377fabce53b996e_r.jpg" data-actualsrc="https://pic3.zhimg.com/v2-5efcb37cba3962cef377fabce53b996e_b.jpg" data-lazy-status="ok"></figure><p>8.SVM预测函数中的b值如何计算？</p><p class="ztext-empty-paragraph"><br></p><p>9.解释核函数的原理，列举常用的核函数。</p><p class="ztext-empty-paragraph"><br></p><p>10.什么样的函数可以作为核函数？</p><p class="ztext-empty-paragraph"><br></p><p>11.解释SMO算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>12.SMO算法如何挑选子问题的优化变量？</p><p class="ztext-empty-paragraph"><br></p><p>13.证明SMO算法中子问题是凸优化问题。</p><p class="ztext-empty-paragraph"><br></p><p>14.证明SMO算法能够收敛。</p><p class="ztext-empty-paragraph"><br></p><p>15.SVM如何解决多分类问题？</p><p class="ztext-empty-paragraph"><br></p><h2><b>第11章 线性模型</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.logistic回归中是否一定要使用logistic函数得到概率值？能使用其他函数吗？</p><p class="ztext-empty-paragraph"><br></p><p>2.名称解释：对数似然比。</p><p class="ztext-empty-paragraph"><br></p><p>3.logistic是线性模型还是非线性模型？</p><p class="ztext-empty-paragraph"><br></p><p>4.logistic回归是生成模型还是判别模型？</p><p class="ztext-empty-paragraph"><br></p><p>5.如果样本标签值为0或1，推导logistic回归的对数似然函数：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cln+L%28%5Cmathbf%7Bw%7D%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%5Cleft%28y_%7Bi%7D+%5Clog+h%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29%2B%5Cleft%281-y_%7Bi%7D%5Cright%29+%5Clog+%5Cleft%281-h%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29%5Cright%29%5Cright%29" alt="[公式]" eeimg="1" data-formula="\ln L(\mathbf{w})=\sum_{i=1}^{l}\left(y_{i} \log h\left(\mathbf{x}_{i}\right)+\left(1-y_{i}\right) \log \left(1-h\left(\mathbf{x}_{i}\right)\right)\right)"> </p><p class="ztext-empty-paragraph"><br></p><p>6.logistic回归中为什么使用交叉熵而不使用欧氏距离作为损失函数？</p><p class="ztext-empty-paragraph"><br></p><p>7.证明logistic回归的优化问题是凸优化问题：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=f%28%5Cmathrm%7Bw%7D%29%3D-%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%5Cleft%28y_%7Bi%7D+%5Clog+h%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29%2B%5Cleft%281-y_%7Bi%7D%5Cright%29+%5Clog+%5Cleft%281-h%5Cleft%28%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29%5Cright%29%5Cright%29" alt="[公式]" eeimg="1" data-formula="f(\mathrm{w})=-\sum_{i=1}^{l}\left(y_{i} \log h\left(\mathbf{x}_{i}\right)+\left(1-y_{i}\right) \log \left(1-h\left(\mathbf{x}_{i}\right)\right)\right)"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>8.推导logistic回归的梯度下降迭代公式。</p><p class="ztext-empty-paragraph"><br></p><p>9.如果类别别标签为+1和-1，推导logistic回归的对数似然函数：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=-%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Clog+%5Cleft%281%2B%5Cexp+%5Cleft%28-y_%7Bi%7D%5Cleft%28%5Cmathrm%7Bw%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathrm%7Bx%7D_%7Bi%7D%2Bb%5Cright%29%5Cright%29%5Cright%29" alt="[公式]" eeimg="1" data-formula="-\sum_{i=1}^{l} \log \left(1+\exp \left(-y_{i}\left(\mathrm{w}^{\mathrm{T}} \mathrm{x}_{i}+b\right)\right)\right)"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>10.写出使用L1和L2正则化项时logistic回归的目标函数。</p><p class="ztext-empty-paragraph"><br></p><p>11.写出softmax回归的预测函数。</p><p class="ztext-empty-paragraph"><br></p><p>12.推导softmax回归的对数似然函数：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7Bl%7D+%5Csum_%7Bj%3D1%7D%5E%7Bk%7D%5Cleft%28y_%7Bi+j%7D+%5Cln+%5Cfrac%7B%5Cexp+%5Cleft%28%5Cboldsymbol%7B%5Ctheta%7D_%7Bj%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%7D+%5Cexp+%5Cleft%28%5Cboldsymbol%7B%5Ctheta%7D_%7Bt%7D%5E%7B%5Cmathrm%7BT%7D%7D+%5Cmathbf%7Bx%7D_%7Bi%7D%5Cright%29%7D%5Cright%29" alt="[公式]" eeimg="1" data-formula="\sum_{i=1}^{l} \sum_{j=1}^{k}\left(y_{i j} \ln \frac{\exp \left(\boldsymbol{\theta}_{j}^{\mathrm{T}} \mathbf{x}_{i}\right)}{\sum_{i=1}^{k} \exp \left(\boldsymbol{\theta}_{t}^{\mathrm{T}} \mathbf{x}_{i}\right)}\right)"> </p><p class="ztext-empty-paragraph"><br></p><p>13.证明softmax回归的优化问题是凸优化问题。</p><p class="ztext-empty-paragraph"><br></p><p>14.推导softmax回归的梯度计算公式。</p><p class="ztext-empty-paragraph"><br></p><p>15.logistic回归如何计算特征的重要性？</p><p class="ztext-empty-paragraph"><br></p><h2><b>第12章 随机森林</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.解释Bagging算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>2.解释随机森林预测算法对分类问题，回归问题的处理。</p><p class="ztext-empty-paragraph"><br></p><p>3.随机森林如何输出特征的重要性？</p><p class="ztext-empty-paragraph"><br></p><p>4.解释随机森林预测算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>5.随机森林为什么能够降低方差？</p><p class="ztext-empty-paragraph"><br></p><h2><b>第13章 Boosting算法</b></h2><p class="ztext-empty-paragraph"><br></p><p>1.写出AdaBoost算法强分类器的预测公式。</p><p class="ztext-empty-paragraph"><br></p><p>2.写出AdaBoost的训练算法。</p><p class="ztext-empty-paragraph"><br></p><p>3.证明强分类器在训练样本集上的错误率上界是每一轮调整样本权重时权重归一化因子的乘积，即下面的不等式成立：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=p_%7Be+r+r+o+r%7D%3D%5Cfrac%7B1%7D%7Bl%7D+%5Csum_%7Bi%3D1%7D%5E%7Bl%7D%5Cleft%5C%7C%5Coperatorname%7Bsgn%7D%5Cleft%28F%5Cleft%28%5Cmathrm%7Bx%7D_%7Bi%7D%5Cright%29%5Cright%29+%5Cneq+y_%7Bi%7D%5Cright%5C%7C+%5Cleq+%5Cprod_%7Bt%3D1%7D%5E%7BT%7D+Z_%7Bt%7D" alt="[公式]" eeimg="1" data-formula="p_{e r r o r}=\frac{1}{l} \sum_{i=1}^{l}\left\|\operatorname{sgn}\left(F\left(\mathrm{x}_{i}\right)\right) \neq y_{i}\right\| \leq \prod_{t=1}^{T} Z_{t}"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>4.证明下面的不等式成立：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Cprod_%7Bt%3D1%7D%5E%7BT%7D+Z_%7Bt%7D%3D%5Cprod_%7Bt%3D1%7D%5E%7BT%7D+2+%5Csqrt%7Be_%7Bt%7D%5Cleft%281-e_%7Bt%7D%5Cright%29%7D%3D%5Cprod_%7Bt%3D1%7D%5E%7BT%7D+%5Csqrt%7B%5Cleft%281-4+%5Cgamma_%7Bt%7D%5E%7B2%7D%5Cright%29%7D+%5Cleq+%5Cexp+%5Cleft%28-2+%5Csum_%7Bt%3D1%7D%5E%7BT%7D+%5Cgamma_%7Bt%7D%5E%7B2%7D%5Cright%29" alt="[公式]" eeimg="1" data-formula="\prod_{t=1}^{T} Z_{t}=\prod_{t=1}^{T} 2 \sqrt{e_{t}\left(1-e_{t}\right)}=\prod_{t=1}^{T} \sqrt{\left(1-4 \gamma_{t}^{2}\right)} \leq \exp \left(-2 \sum_{t=1}^{T} \gamma_{t}^{2}\right)"> </p><p class="ztext-empty-paragraph"><br></p><p>5.简述广义加法模型的原理。</p><p class="ztext-empty-paragraph"><br></p><p>6.离散型AdaBoost的损失函数是什么函数？</p><p class="ztext-empty-paragraph"><br></p><p>7.从广义加法模型和指数损失函数推导AdaBoost的训练算法。</p><p class="ztext-empty-paragraph"><br></p><p>8.解释实数型AdaBoost算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>9.AdaBoost算法的弱分类器应该如何选择？</p><p class="ztext-empty-paragraph"><br></p><p>10.简述梯度提升算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>11.假设使用均方误差函数，梯度提升算法如何解决回归问题？</p><p class="ztext-empty-paragraph"><br></p><p>12.梯度提升算法如何解决二分类问题？</p><p class="ztext-empty-paragraph"><br></p><p>13.对于多分类问题，梯度提升算法的预测函数是 <img src="https://www.zhihu.com/equation?tex=F_%7Bk%7D%28x%29" alt="[公式]" eeimg="1" data-formula="F_{k}(x)"> 。样本属于每个类的概率为：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=p_%7Bk%7D%28%5Cmathbf%7Bx%7D%29%3D%5Cfrac%7B%5Cexp+%5Cleft%28F_%7Bk%7D%28%5Cmathbf%7Bx%7D%29%5Cright%29%7D%7B%5Csum_%7Bl%3D1%7D%5E%7BK%7D+%5Cexp+%5Cleft%28F_%7Bl%7D%28%5Cmathbf%7Bx%7D%29%5Cright%29%7D" alt="[公式]" eeimg="1" data-formula="p_{k}(\mathbf{x})=\frac{\exp \left(F_{k}(\mathbf{x})\right)}{\sum_{l=1}^{K} \exp \left(F_{l}(\mathbf{x})\right)}"> </p><p class="ztext-empty-paragraph"><br></p><p>如果加上限制条件：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bl%3D1%7D%5E%7BK%7D+F_%7Bl%7D%28%5Cmathbf%7Bx%7D%29%3D0" alt="[公式]" eeimg="1" data-formula="\sum_{l=1}^{K} F_{l}(\mathbf{x})=0"> </p><p class="ztext-empty-paragraph"><br></p><p>证明如下结论成立：</p><p class="ztext-empty-paragraph"><br></p><p><img src="https://www.zhihu.com/equation?tex=F_%7Bk%7D%28%5Cmathbf%7Bx%7D%29%3D%5Cln+p_%7Bk%7D%28%5Cmathbf%7Bx%7D%29-%5Cfrac%7B1%7D%7BK%7D+%5Csum_%7Bl%3D1%7D%5E%7BK%7D+%5Cln+p_%7Bl%7D%28%5Cmathbf%7Bx%7D%29" alt="[公式]" eeimg="1" data-formula="F_{k}(\mathbf{x})=\ln p_{k}(\mathbf{x})-\frac{1}{K} \sum_{l=1}^{K} \ln p_{l}(\mathbf{x})"> </p><p class="ztext-empty-paragraph"><br></p><p class="ztext-empty-paragraph"><br></p><p>14.解释XGBoost算法的原理。</p><p class="ztext-empty-paragraph"><br></p><p>15.XGBoost算法为何要泰勒展开到二阶？</p><p></p></div>
</section>
    

    <footer id="post-meta" class="clearfix">
        <a href="/about/">
        <img class="avatar" src="/images/avatar.png">
        <div>
            <span class="dark">BouSeng</span>
            <span>What I cannot create, I do not understand</span>
        </div>
        </a>
        <section id="sharing">
            <a title="Share to Twitter" class="twitter" href="https://twitter.com/intent/tweet?text=bouseng.com/2019/10/18/深度学习习题/ - 深度学习习题 @"><span class="icon-twitter">tweet</span></a>
            <a title="Share to Facebook" class="facebook" href="#" onclick="
                window.open(
                  'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
                  'facebook-share-dialog',
                  'width=626,height=436');
                return false;"><span class="icon-facebook-sign">Share</span>
            </a>
        </section>
    </footer>


  <section id="comment">
    <button class="btn" id="loadcmts" onclick="cmts.load();">加载评论</button>
    <div id="gitment"></div>
    <script src='/js/gitment.browser.js'></script>
    <link rel="stylesheet" href=''>
    <script>
      var cmts={
        load:function cmts(){
          var gitment = new Gitment({
          
            id: "深度学习习题",
          
            owner: "",
            repo: "",
            oauth: {
              client_id: "",
              client_secret: "",
            },
          })
          gitment.render('gitment');
          var loadcmt = document.getElementById("loadcmts");
          var imyourfather = loadcmt.parentNode;
          imyourfather.removeChild(loadcmts)
        }
      }
    </script>
  </section>


	<footer id="footer">
	<div id="social">
		<p class="small">©  BouSeng Chan| Powered by Hexo & 
			<a href="https://github.com/F0r3at/Lights"> Lights</a>
		</p>
	</div>
</footer>

</section>

	<script src="//cdnjs.loli.net/ajax/libs/instantclick/3.0.1/instantclick.min.js" data-no-instant></script>
	<script data-no-instant>
		
		InstantClick.init('mousedown');
	</script>



